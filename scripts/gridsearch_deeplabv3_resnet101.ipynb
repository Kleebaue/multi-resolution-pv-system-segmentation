{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1471b281-bbc8-43f8-b6ad-6cc59bdc9779",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparamter Tuning for Multi Resolution Solar Photovoltaik System Segmentation Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2674e496-5c91-4dd1-9152-c5a9c65224fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as F, ToTensor, InterpolationMode\n",
    "import torchvision.models as models\n",
    "from torchvision.models.segmentation import deeplabv3_resnet101, DeepLabV3_ResNet101_Weights \n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, root_dirs, resize=256, crop_size=256):\n",
    "        self.root_dirs = root_dirs\n",
    "        self.image_dirs = [os.path.join(root_dir, 'image') for root_dir in root_dirs]\n",
    "        self.mask_dirs = [os.path.join(root_dir, 'mask') for root_dir in root_dirs]\n",
    "        self.image_filenames = self.collect_image_filenames()\n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.CenterCrop(crop_size),\n",
    "            transforms.Resize([resize], interpolation=InterpolationMode.BILINEAR),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def collect_image_filenames(self):\n",
    "        image_filenames = []\n",
    "        for image_dir in self.image_dirs:\n",
    "            image_filenames += os.listdir(image_dir)\n",
    "        return image_filenames\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_filenames[idx]\n",
    "        image_path = self.find_image_path(image_name)\n",
    "        mask_path = self.find_mask_path(image_name)\n",
    "\n",
    "        image = self.load_image(image_path)\n",
    "        mask = self.load_mask(mask_path)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "    def find_image_path(self, image_name):\n",
    "        for image_dir in self.image_dirs:\n",
    "            image_path = os.path.join(image_dir, image_name)\n",
    "            if os.path.exists(image_path):\n",
    "                return image_path\n",
    "        raise FileNotFoundError(f\"Image file not found: {image_name}\")\n",
    "\n",
    "    def find_mask_path(self, image_name):\n",
    "        for mask_dir in self.mask_dirs:\n",
    "            mask_path = os.path.join(mask_dir, image_name)\n",
    "            if os.path.exists(mask_path):\n",
    "                return mask_path\n",
    "        raise FileNotFoundError(f\"Mask file not found: {image_name}\")\n",
    "\n",
    "    def load_image(self, path):\n",
    "        image = Image.open(path).convert('RGB')\n",
    "        image = self.transforms(image)\n",
    "        return image\n",
    "\n",
    "    def load_mask(self, path):\n",
    "        mask = Image.open(path).convert('L')\n",
    "        mask = self.transforms(mask)\n",
    "        mask = torch.squeeze(mask, dim=0)  # Squeeze the mask\n",
    "        return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb819529-6c94-411d-a9ad-b7a2656313ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: Devices: 4 A100-SXM4-40GB GPU available, will use gpu!\n",
      "Number of CPU cores: 128\n"
     ]
    }
   ],
   "source": [
    "use_cuda = True\n",
    "\n",
    "if use_cuda and not torch.cuda.is_available():\n",
    "    print(\"Error: cuda requested but not available, will use cpu instead!\")\n",
    "    device = torch.device('cpu')\n",
    "elif not use_cuda:\n",
    "    print(\"Info: will use cpu!\")\n",
    "    device = torch.device('cpu')\n",
    "else:\n",
    "    print(f\"Info: Devices: {torch.cuda.device_count()} {torch.cuda.get_device_name(0)} GPU available, will use gpu!\")\n",
    "    device = torch.device('cuda')\n",
    "    \n",
    "print(f\"Number of CPU cores: {os.cpu_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "367c2ce1-4390-4a62-9190-75f19466d0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torch.nn as nn # nn.CrossEntropyLoss, nn.MSELoss, nn.BCEWithLogitsLoss, nn.HuberLoss\n",
    "import torchvision.ops as ops # ops.complete_box_iou_loss, ops.distance_box_iou, ops.generalized_box_iou\n",
    "import torch.optim as optim # optim.Adagrad, Adam, RMSprop, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b65dccc-398c-4c1a-b8a7-3b91c9e3f7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.classification import BinaryAccuracy, BinaryRecall, BinaryPrecision, BinaryF1Score, BinaryJaccardIndex\n",
    "calculate_accuracy = BinaryAccuracy()\n",
    "calculate_precision = BinaryPrecision()\n",
    "calculate_recall = BinaryRecall()\n",
    "calculate_f1_score = BinaryF1Score()\n",
    "calculate_iou = BinaryJaccardIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a9f5bab-8a92-4cb9-92c5-1eed1cc27be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed paramters\n",
    "batch_size = 16\n",
    "num_epochs = 100\n",
    "\n",
    "# search space parameters\n",
    "search_space = {'learning_rate':[0.001, 0.0001, 0.00001],\n",
    "                'loss_function':[nn.HuberLoss, nn.BCEWithLogitsLoss, nn.MSELoss, nn.CrossEntropyLoss],\n",
    "                'optimiser':[optim.Adagrad, optim.Adam, optim.RMSprop, optim.SGD],\n",
    "               # 'batch_size':[2, 4, 8, 12, 16, 18, 24, 32, 48, 64], \n",
    "               # 'stride':[2, 3, 4, 5]\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "767a3c57-a6aa-47dc-9fcb-9204de567af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import csv\n",
    "\n",
    "def train_with_hyperparameters(model, dataset, dataset_eval, num_epochs, batch_size, learning_rate, save_path, log_dir, dataset_paths, loss_function, optimizer):\n",
    "    # Instantiate the DeepLabV3_ResNet101 model with pretrained weights\n",
    "   \n",
    "    # Define the loss function and optimizer\n",
    "    criterion = loss_function()\n",
    "    optimizer = optimizer(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Create the data loader\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Create the data loader for evaluation\n",
    "    eval_dataloader = DataLoader(dataset_eval, batch_size=100, shuffle=False)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        running_loss = 0.0\n",
    "        model.train()\n",
    "\n",
    "        for images, masks in dataloader:\n",
    "            # Move images and masks to the device\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)['out']\n",
    "            outputs = torch.squeeze(outputs)  # Remove the extra dimensions\n",
    "\n",
    "            loss = criterion(outputs, masks)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(dataset)\n",
    "        print('Loss: {:.4f}'.format(epoch_loss))\n",
    "\n",
    "        best_iou = 0.0  # Variable to track the highest IoU\n",
    "        best_accuracy = 0.0\n",
    "        best_precision = 0.0\n",
    "        best_recall = 0.0\n",
    "        best_f1_score = 0.0\n",
    "        best_epoch = 0\n",
    "    \n",
    "        # Evaluate the model and plot the images, masks, and predicted masks every 10th epoch\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                accuracy_list = []\n",
    "                precision_list = []\n",
    "                recall_list = []\n",
    "                f1_score_list = []\n",
    "                iou_list = []\n",
    "                best_epoch = []\n",
    "                \n",
    "                for images, masks in eval_dataloader:\n",
    "                    images = images.to(device)\n",
    "                    masks = masks.to(device)\n",
    "\n",
    "                    outputs = model(images)['out']\n",
    "                    predicted_masks = torch.sigmoid(outputs) > 0.5\n",
    "\n",
    "                    images = images.cpu()\n",
    "                    masks = masks.cpu()\n",
    "                    predicted_masks = predicted_masks.cpu()\n",
    "\n",
    "                    threshold = 0.5\n",
    "                    mask = (masks >= threshold).int()\n",
    "                    predicted_mask = (predicted_masks >= threshold).int()\n",
    "\n",
    "                    accuracy = calculate_accuracy(predicted_mask.squeeze(), mask)\n",
    "                    precision = calculate_precision(predicted_mask.squeeze(), mask)\n",
    "                    recall = calculate_recall(predicted_mask.squeeze(), mask)\n",
    "                    f1_score = calculate_f1_score(predicted_mask.squeeze(), mask)\n",
    "                    iou = calculate_iou(predicted_mask.squeeze(), mask)\n",
    "\n",
    "                    accuracy_list.append(accuracy)\n",
    "                    precision_list.append(precision)\n",
    "                    recall_list.append(recall)\n",
    "                    f1_score_list.append(f1_score)\n",
    "                    iou_list.append(iou)\n",
    "\n",
    "                # Calculate average metrics\n",
    "                avg_accuracy = sum(accuracy_list) / len(accuracy_list)\n",
    "                avg_precision = sum(precision_list) / len(precision_list)\n",
    "                avg_recall = sum(recall_list) / len(recall_list)\n",
    "                avg_f1_score = sum(f1_score_list) / len(f1_score_list)\n",
    "                avg_iou = sum(iou_list) / len(iou_list)\n",
    "\n",
    "                # Save the trained model\n",
    "                if avg_iou >= best_iou:\n",
    "                    best_iou = avg_iou\n",
    "                    best_accuracy = avg_accuracy\n",
    "                    best_precision = avg_precision\n",
    "                    best_recall = avg_recall\n",
    "                    best_f1_score = avg_f1_score \n",
    "                    best_epoch = epoch + 1\n",
    "                \n",
    "                else:\n",
    "                    break\n",
    "                    \n",
    "    print('Accuracy: {:.4f}'.format(best_accuracy))\n",
    "    print('Precision: {:.4f}'.format(best_precision))\n",
    "    print('Recall: {:.4f}'.format(best_recall))\n",
    "    print('F1 Score: {:.4f}'.format(best_f1_score))\n",
    "    print('IoU: {:.4f}'.format(best_iou))\n",
    "    \n",
    "    return best_accuracy.item(), best_precision.item(), best_recall.item(), best_f1_score.item(), best_iou.item(), best_epoch\n",
    "\n",
    "def train_segmentation_network(model, dataset, dataset_eval, num_epochs, batch_size, save_path, log_dir, dataset_paths):\n",
    "    # Generate all combinations of hyperparameters\n",
    "    hyperparameter_combinations = list(itertools.product(search_space['learning_rate'], search_space['loss_function'], search_space['optimiser']))\n",
    "\n",
    "    # Create a CSV file to store the results\n",
    "    csv_file = open(save_path + 'hyper_training_results_add.csv', 'w', newline='')\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow(['Loss Function', 'Optimiser', 'Learning Rate', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'IoU', 'best_epoch'])\n",
    "\n",
    "    # Iterate over the hyperparameter combinations\n",
    "    for hyperparameters in hyperparameter_combinations:\n",
    "        learning_rate, loss_function, optimiser = hyperparameters\n",
    "\n",
    "        # Print the current hyperparameters\n",
    "        print('Training with hyperparameters:')\n",
    "        print('Learning Rate:', learning_rate)\n",
    "        print('Loss Function:', loss_function.__name__)\n",
    "        print('Optimiser:', optimiser.__name__)\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Load the model for each combination of hyperparameters\n",
    "        model = load_model()\n",
    "\n",
    "        # Train the model with the current hyperparameters\n",
    "        accuracy, precision, recall, f1_score, iou, best_epoch = train_with_hyperparameters(model, dataset, dataset_eval, num_epochs, batch_size, learning_rate, save_path, log_dir, dataset_paths, loss_function, optimiser)\n",
    "\n",
    "        # Write the results to the CSV file\n",
    "        writer.writerow([loss_function.__name__, optimiser.__name__, learning_rate, accuracy, precision, recall, f1_score, iou, best_epoch])\n",
    "\n",
    "    # Close the CSV file\n",
    "    csv_file.close()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2263ea4-ac94-40ab-bee4-f38852a3babd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and prepare model\n",
    "def load_model():\n",
    "    model = models.segmentation.deeplabv3_resnet101(weights=DeepLabV3_ResNet101_Weights.COCO_WITH_VOC_LABELS_V1)\n",
    "    model.backbone.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    num_classes = 1  # Assuming binary segmentation (1 class)\n",
    "    model.classifier = models.segmentation.deeplabv3.DeepLabHead(2048, num_classes)\n",
    "\n",
    "        # Set up the device (CPU or GPU)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77d53e69-c4ee-4028-92fa-24fc8271332d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path1 = '/share/data1/mkleebauer/pv_segmentation/PV01_train'\n",
    "val_path1 = '/share/data1/mkleebauer/pv_segmentation/PV01_val'\n",
    "\n",
    "#dataset_path1 = '/share/data1/mkleebauer/pv_segmentation/mini_train'\n",
    "#val_path1 = '/share/data1/mkleebauer/pv_segmentation/mini_eval'\n",
    "\n",
    "dataset_path = '/share/data1/mkleebauer/pv_segmentation/'\n",
    "save_path = dataset_path \n",
    "log_dir = dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f814b04-211d-45ad-bcee-4f9eea836d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_paths = [dataset_path1]\n",
    "#dataset_paths = [dataset_path1, dataset_path2, dataset_path3]\n",
    "dataset = SegmentationDataset(dataset_paths)\n",
    "\n",
    "dataset_path_eval = [val_path1]\n",
    "dataset_eval = SegmentationDataset(dataset_path_eval)\n",
    "\n",
    "model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9123e4-1c93-4af2-a120-a79a79dedccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_segmentation_network(model, dataset, dataset_eval, num_epochs, batch_size, save_path, log_dir, dataset_paths)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "f3b09f0dae079356b11e2992c8ce1698bd60fda55aea4c87f004ec164747e9c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
